{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBA TECNICA: VEHICULOS AUTONOMOS#\n",
    "\n",
    "## Datos no estructurados - Millenium BPO ##\n",
    "\n",
    "## Camilo Garcia Torres ##\n",
    "## 05 de Febrero del 2020 ##\n",
    "\n",
    "A continuación se presentara el codigo utilizado para la prueba tecnica asignada por la empresa Millenium BPO en su proceso de selección de personal.\n",
    "\n",
    "\n",
    "1. El primer paso es abrir la terminal o Command Shell en el que se este utilizando ROS y no olvidar la linea de codigo .bash para permitir la funcionalidad del mismo. \n",
    "\n",
    "2. El siguiente paso es utilizar roscore para permitir que los nodos disponibles se puedan comunicar entre si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ source /opt/ros/melodic/setup.bash\n",
    "$ roscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Una vez el roscore este activo, abrimos otro Command Shell y procedemos a utilizar el paquete de rosbag. Para esto utilizaremos el comando rosbag play para darle inicio a la grabación. El subfijo -l es utilizado para dejar la grabación en un loop, pero no es necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ rosbag play -l 2020-01-29-11-19-28.bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Output del comando deberia verse como la imagen a continuación. El terminal de la izquierda muestra el output de roscore, el terminal de la derecha muestra el output despus de utilizar rosbag play\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"rosbag.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ahora utilizaremos el paquete de rviz donde podremos visualizar el robot en un plano 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$rviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra un video donde se explica los pasos a seguir para observar la localización instantanea del robot en el archivo .bag. Es necesario primero agregar el mapa, la posicion inicial presente en el paquete y el marco de referencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"rviz.mp4\" controls  width=\"1000\"  height=\"500\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"rviz.mp4\" , width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> 5. Por otro lado, si queremos ver la posicion instantanea en terminos de coordenadas podemos lograrlo por medio de otro paquete llamado tf. En un nuevo command shell y con la funcion tf_echo podemos ver las coordenedas instantaneas que se presentan entre 2 marcos de referencia diferentes.  Ademas, tambien podemos ver los tipos de mensajes disponibles en el rosbag por medio de rostpic list. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ rostopic list\n",
    "$ rosrun tf tf_echo /base_link /laser   #marco de referencia del laser con respecto al mapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tfbaselaser.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Segun el codigo podemos ver que estamos comparando el marco de referencia entre la base del robot y la posición del laser. Si miramos la imagen podemos ver que los valores de las coordenadas no cambian debido a que ambas partes conforman el robot y el marco de referencia del laser solo esta trasladado un poco con respecto al marco de referencia de la base. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: justify\"> Tambien podemos comparar otros 2 marcos de referencia, como por ejemplo marco de referencia del laser con el marco de referencia sobre el mapa. Podemos ver como los valores instantaneos cambian debido a que el robot se esta moviendo. Para esto utilizaremos la misma funcion de antes pero cambiando los valores por /map /laser. </div>\n",
    "\n",
    "<img src=\"tfmaplaser.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, podemos ver las dependencias entre los marcos de referencia presentes en el robot (y sus relaciones entre ellas) por medio de tf view_frames y guardar la imagen como un archivo .png en nuestro computador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ rosrun tf view_frames\n",
    "$ evince frames.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"framestree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"text-align: justify\"> Este arbol permite ver las dependencias de los marcos de referencia del robot. Como podemos ver, todos los sensores, lasers y camara son dependientes de la base del robot (locales). Mientras que la base misma depende del marco de referencia del mapa (global). Por ultimo, no se utilizo la odometria ni los datos de /odom ni /initial pose directamente, pero rviz los presenta en un procreso llamado SLAM (simultaneous localization and mapping) en el cual se utiliza valores anteriores para realizar el proceso de localizacion del robot de una manera mas precisa.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
